#!/usr/bin/env python
# -*- mode: landisk-python; coding: utf-8; -*-
# vim:ts=4 sw=4 sts=4 ai si et sta

from __future__ import with_statement

from ..fileaccess import (MODE_READ, MODE_WRITE, ST_MODE_DIR, ST_MODE_FILE,
    DEFAULT_LOCALE, DEBUG)
from ionas.exception import *
from filelikeobj import FileLikeObj, FilePart
from localaccess import local_except_handler

import os
import sys
import stat
import locale
import math
import errno
import time
import httplib
try:
    import email.utils as emailut
except:
    import email.Utils as emailut

from boto.s3.connection import S3Connection, S3ResponseError
from boto.s3.multipart import part_lister
from boto.s3.bucketlistresultset import multipart_upload_lister
from boto.utils import compute_md5

INVALID_ACCESS_KEY_ID = "InvalidAccessKeyId"
SIGNATURE_DOES_NOT_MATCH = "SignatureDoesNotMatch"
NO_SUCH_BUCKET = "NoSuchBucket"
NO_SUCH_KEY = "NoSuchKey"
NO_SUCH_UPLOAD = "NoSuchUpload"
ACCESS_DENIED = "AccessDenied"
KEY_TOO_LONG = "KeyTooLong"
ENTITY_TOO_LARGE = "EntityTooLarge"

PRIORITY_NONE = "none"
PRIORITY_FILE = "file"
PRIORITY_DIR = "dir"

UP_MIN_CHUNK_SIZE = 100 * 1024 * 1024
UP_MAX_CHUNK_AMOUNT = 10000
DL_CHUNK_SIZE = 100 * 1024 * 1024
CANCEL_RETRY_NUM = 8
MPUPLOAD_RETRY_NUM = 0
DOWNLOAD_RETRY_NUM = 3

SIZE_IDX = 2

AMAZONS3_CANCEL_ERRS = (NO_SUCH_KEY, NO_SUCH_UPLOAD,
                        ACCESS_DENIED, KEY_TOO_LONG,
                        ENTITY_TOO_LARGE)
LOCAL_CANCEL_ERRS = (errno.EACCES, errno.EPERM,
                     errno.ENOENT, errno.EEXIST,
                     errno.ENAMETOOLONG)

AMAZONS3_SEP = "/"


def amazons3_except_handler(exc_ins):
    tb_info = sys.exc_info()[2]

    # amazons3filetreeのget_treeから
    # 呼ばれた場合のためにinstance check
    if not isinstance(exc_ins, S3ResponseError):
        raise exc_ins, None, tb_info

    if exc_ins.code == ACCESS_DENIED:
        raise PermissionError(exc_ins)
    if exc_ins.code == INVALID_ACCESS_KEY_ID:
        raise InvalidIdentifierError(exc_ins)
    if exc_ins.code == SIGNATURE_DOES_NOT_MATCH:
        raise InvalidPasswordError(exc_ins)
    if exc_ins.code == NO_SUCH_BUCKET:
        raise NoRootEntryError(exc_ins)
    if exc_ins.code == NO_SUCH_KEY or exc_ins.code == NO_SUCH_UPLOAD:
        raise NoEntryError(exc_ins)
    if exc_ins.code == KEY_TOO_LONG:
        raise NameTooLongError(exc_ins)
    if exc_ins.code == ENTITY_TOO_LARGE:
        raise SizeTooLargeError(exc_ins)

    raise exc_ins, None, tb_info


def get_part_list(mp):
    part_list = []
    for part in part_lister(mp):
        part_list.append(part)
    return part_list


_DEF_FUNC = lambda *args, **kwargs: None


def _is_cancel_errs(ins):

    if isinstance(ins, S3ResponseError):
        if ins.code in AMAZONS3_CANCEL_ERRS:
            return True
    if isinstance(ins, (IOError, OSError)):
        if ins.errno in LOCAL_CANCEL_ERRS:
            return True
    if isinstance(ins, ModifiedInTransferring):
        return True
    return False


def _is_cancel_retry_errs(ins):

    if isinstance(ins, S3ResponseError):
        if ins.code in (NO_SUCH_KEY, NO_SUCH_UPLOAD):
            return True
    return False


def _upload_part(mp, fp, part_num, file_like, upload_cbs={}):

    def _exec_upload_part(retry_num=MPUPLOAD_RETRY_NUM):
        try:
            mp.upload_part_from_file(fp=fp, part_num=part_num)
        except Exception, ins:
            if retry_num:
                _exec_upload_part(retry_num - 1)
            else:
                raise ins

    _exec_upload_part()


def _mpupload(mp, file_like, uploaded_part_list=[], upload_cbs={}):

    src_file_stat = file_like.get_stat()
    src_size = src_file_stat.st_size
    src_ctime = src_file_stat.st_ctime

    chunk_amount = int(math.ceil(src_size / float(UP_MIN_CHUNK_SIZE)))

    if chunk_amount > UP_MAX_CHUNK_AMOUNT:
        chunk_size = int(math.ceil(src_size / float(UP_MAX_CHUNK_AMOUNT)))
        chunk_amount = UP_MAX_CHUNK_AMOUNT
    else:
        chunk_size = UP_MIN_CHUNK_SIZE

    uploaded_part_num_list = []
    for uploaded_part in uploaded_part_list:
        uploaded_part_num_list.append(uploaded_part.part_number)

    for i in range(chunk_amount):
        part_num = i + 1

        if part_num in uploaded_part_num_list:
            continue

        offset = i * chunk_size
        remaining_bytes = src_size - offset
        bytes = min([chunk_size, remaining_bytes])

        fp = FilePart(file_like, offset, bytes)

        cur_file_stat = file_like.get_stat()
        cur_size = cur_file_stat.st_size
        cur_ctime = cur_file_stat.st_ctime

        try:
            if src_size != cur_size or src_ctime != cur_ctime:
                raise ModifiedInTransferring()
            _upload_part(mp, fp, part_num, file_like, upload_cbs)
        except Exception, ins:
            if _is_cancel_errs(ins):
                """
                一時的にNoSuchKeyかNoSuchUploadが
                かえってくる場合があり、
                その場合はしばらくリトライする
                """
                for i in range(CANCEL_RETRY_NUM):
                    try:
                        mp.cancel_upload()
                    except Exception, ins:
                        if not _is_cancel_retry_errs(ins):
                            raise ins
                        time.sleep(i * i)
                    else:
                        break

                upload_cbs.get("post_cancel", _DEF_FUNC)(file_like)
            raise ins

    if len(get_part_list(mp)) == chunk_amount:
        mp.complete_upload()
        upload_cbs.get("post_upload", _DEF_FUNC)(file_like)
    else:
        mp.cancel_upload()
        upload_cbs.get("post_cancel", _DEF_FUNC)(file_like)


class AmazonS3FileObj(FileLikeObj):

    def __init__(self, file, path, rrs=False,
                 bucket=None, key_path="", mpupload=True,
                 upload_cbs={}):

        self.rrs = rrs
        self.bucket = bucket
        self.key_path = key_path
        self.mpupload = mpupload
        self.upload_cbs = upload_cbs
        self.sep = AMAZONS3_SEP
        FileLikeObj.__init__(self, file, path)

    def __len__(self):

        return self.file.size

    def tell(self):

        # COMMENT read完了しても0にならないが問題あるか?
        # とりあえず不使用なのでNotImplement
        #return self.point
        raise NotImplementedError

    def seek(self, offset, whence=os.SEEK_SET):

        # COMMENT エラー処理どうする
        # writeの場合のpointはサポートしないがよいか
        # とりあえず不使用なのでNotImplement
        #if whence == os.SEEK_SET:
        #    self.point = offset
        #elif whence == os.SEEK_CUR:
        #    self.point += offset
        #elif whence == os.SEEK_END:
        #    self.point = len(self) - offset
        raise NotImplementedError

    def get_path(self):

        raise NotImplementedError

    def read(self, size):

        try:
            return self.file.read(size)
        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def write_by_path(self, file_like):

        try:
            if (self.mpupload and file_like.get_size() >= UP_MIN_CHUNK_SIZE):

                self.file, uploaded_part_list = self.upload_cbs.\
                    get("get_mp", _DEF_FUNC)\
                    (self.key_path, self.rrs, file_like)

                if not self.file:
                    return

                self.upload_cbs.get("set_mp", _DEF_FUNC)(self.file, file_like)
                _mpupload(self.file, file_like,
                          uploaded_part_list, self.upload_cbs)

            else:

                self.file = self.bucket.new_key(self.key_path)
                md5 = compute_md5(file_like)
                self.upload_cbs.get("pre_upload", _DEF_FUNC)(file_like)
                self.file.set_contents_from_file(
                    file_like, reduced_redundancy=self.rrs,
                    md5=md5, size=md5[SIZE_IDX])

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

        except (IOError, OSError), ins:
            local_except_handler(ins, file_like.get_path())

    def read_by_path(self, file_like):

        bucket_name, key_path = self.path.strip(self.sep).split(self.sep, 1)

        def reopen():
            self.file = self.bucket.get_key(key_path)
            if not self.file:
                raise NoEntryError

        try:
            size = self.file.size
            mtime = int(emailut.mktime_tz(
                emailut.parsedate_tz(
                    self.file.last_modified)))

            if size == 0:
                self.file.get_file(file_like.file)
                return

            headers = {"Range": ""}
    
            chunk_num = 0
            retry_num = DOWNLOAD_RETRY_NUM

            while True:
                start = DL_CHUNK_SIZE * chunk_num
                end = start + DL_CHUNK_SIZE - 1
                headers["Range"] = "bytes=%d-%d" % (start, end)

                try:
                    self.file.get_file(file_like.file, headers)
                except httplib.IncompleteRead, err:
                    if not retry_num:
                        raise err
                    file_like.seek(start)
                    reopen()
                    retry_num -= 1
                    continue

                if end >= size - 1:
                    break
                chunk_num += 1
                retry_num = DOWNLOAD_RETRY_NUM

            cur_key = self.bucket.get_key(key_path)
            if not cur_key:
                raise NoEntryError

            cur_size = cur_key.size
            cur_mtime = int(emailut.mktime_tz(
                emailut.parsedate_tz(
                    cur_key.last_modified)))

            if size != cur_size or mtime != cur_mtime:
                raise ModifiedInTransferring()

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

        except (IOError, OSError), ins:
            local_except_handler(ins, file_like.get_path())

    def close(self):

        if hasattr(self.file, "close"):
            self.file.close()


class AmazonS3Access():

    def __init__(self, key, secret, bucket="",
        encoding="", is_secure=True, rrs=False,
        proxy_host=None, proxy_port=None,
        validate=False, same_name_priority=PRIORITY_DIR,
        host=None, port=None, mpupload=True):

        self.sep = AMAZONS3_SEP

        if encoding:
            self.encoding = encoding
        else:
            self.encoding = locale.getdefaultlocale()[1]
            if not self.encoding:
                self.encoding = DEFAULT_LOCALE

        self.rrs = rrs
        self.same_name_priority = same_name_priority
        self.mpupload = mpupload

        if not host:
            host = S3Connection.DefaultHost

        try:
            self.__conn = S3Connection(key, secret,
                is_secure=is_secure, proxy=proxy_host, proxy_port=proxy_port,
                host=host, port=port)
            if bucket:
                self.bucket = self.__conn.get_bucket(bucket, validate)
            else:
                self.bucket = ""

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def listdir(self, path):
        # COMMENT XR 1.55では当メソッド不使用
        return

        try:
            # 文字コード大丈夫か
            path_list = path.strip(
                self.sep).rstrip(self.sep).split(self.sep, 1)
            # 自分は本当に0要素目か
            bucket_name = path_list[0]
            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket_name)
            if len(path_list) == 2:
                key_path = path_list[1]
                keys = bucket.get_all_keys(prefix=key_path + self.sep)[1:]
                return [
                    key.name.replace(key_path + self.sep, "", 1).rstrip(
                    self.sep).encode(self.encoding)
                    for key
                    in keys
                    if len(key.name.replace(key_path + self.sep, "", 1).rstrip(
                        self.sep).split(self.sep)) < 2]
            elif len(path_list) == 1:
                keys = bucket.get_all_keys()
                return [
                    key.name.split(self.sep)[0].encode(self.encoding)
                    for key
                    in keys
                    if len(key.name.rstrip(self.sep).split(self.sep)) < 2]
            # path="/"などの場合にget_all_buckets()などするか?
            # elseはlen(path_list) == 1を想定している
            # それ以外の場合の処理未実装。必要か
        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def isdir(self, entry):  # COMMENT 高速化するなら必要かも
        pass

    def open(self, path, mode, upload_cbs={}):
        try:
            bucket_name, key_path = path.strip(self.sep).split(self.sep, 1)
            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket_name)

            if mode == MODE_READ:
                target = bucket.get_key(key_path)
                if not target:
                    raise NoEntryError
                return AmazonS3FileObj(target, path,
                                       bucket=bucket,
                                       upload_cbs=upload_cbs)

            if mode == MODE_WRITE:
                return AmazonS3FileObj(None, path, self.rrs,
                                       bucket, key_path, self.mpupload,
                                       upload_cbs=upload_cbs)

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def __get_stat_priority_dir(self, bucket, key_path):
        key = bucket.get_key(key_path + self.sep)
        if not key:
            key = bucket.get_key(key_path)
            if not key:
                raise NoEntryError
            """ This Is File. """
            st_mode = ST_MODE_FILE
            st_mtime = int(emailut.mktime_tz(
                emailut.parsedate_tz(
                    key.last_modified)))
            st_size = key.size
        else:
            st_mode = ST_MODE_DIR
            st_mtime = int(emailut.mktime_tz(
                emailut.parsedate_tz(
                    key.last_modified)))
            st_size = None
        return (st_mode, st_mtime, st_size)

    def __get_stat_priority_file(self, bucket, key_path):
        key = bucket.get_key(key_path)
        if not key:
            key = bucket.get_key(key_path + self.sep)
            if not key:
                raise NoEntryError
            """ This Is File. """
            st_mode = ST_MODE_DIR
            st_mtime = emailut.mktime_tz(
                emailut.parsedate_tz(
                    key.last_modified))
            st_size = None
        else:
            st_mode = ST_MODE_FILE
            st_mtime = emailut.mktime_tz(
                emailut.parsedate_tz(
                    key.last_modified))
            st_size = key.size
        return (st_mode, st_mtime, st_size)

    def __get_stat(self, bucket, key_path):
        if self.same_name_priority == PRIORITY_FILE:
            return self.__get_stat_priority_file(bucket, key_path)
        if self.same_name_priority == PRIORITY_DIR:
            return self.__get_stat_priority_dir(bucket, key_path)
        else:
            # COMMENT 他にパターンあれば実装
            return self.__get_stat_priority_dir(bucket, key_path)

    def stat(self, path):
        try:
            path_list = path.strip(self.sep).split(self.sep, 1)
            if self.bucket:
                bucket = self.bucket
            else:
                bucket_name = path_list[0]
                bucket = self.__conn.get_bucket(bucket_name)

            if len(path_list) == 2:
                st_mode, st_mtime, st_size = self.__get_stat(
                    bucket, path_list[1])
            elif len(path_list) == 1:
                # COMMENT bucketはDIR扱い
                st_mode = ST_MODE_DIR
                st_mtime = None
                st_size = 0

            return {
                stat.ST_MODE: st_mode,
                stat.ST_MTIME: st_mtime,
                stat.ST_SIZE: st_size}

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    # COMMENT nasdsyncはupload,download両方共localをutimeしている。
    # dropbox側はutimeできない。
    # 時刻を基準にファイル更新判断しているので、
    # local側を更新することにより時刻一致させている。
    #def utime(self, path, mtime):
    #    pass

    def remove(self, path):
        try:
            bucket_name, key_path = path.strip(self.sep).split(self.sep, 1)
            # COMMENT key_pathもある前提

            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket_name)

            bucket.delete_key(key_path)

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def rmdir(self, path):
        try:
            path_list = path.strip(
                self.sep).rstrip(self.sep).split(self.sep, 1)

            if self.bucket:
                bucket = self.bucket
            else:
                bucket_name = path_list[0]
                bucket = self.__conn.get_bucket(bucket_name)

            if len(path_list) == 2:
                key_path = path_list[1]
                key_list = bucket.get_all_keys(prefix=key_path + self.sep)
                bucket.delete_keys(key_list)
            elif len(path_list) == 1:
                bucket.delete_keys(bucket.get_all_keys())
                bucket.delete()

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def mkdir(self, path):
        try:
            path_list = path.strip(
                self.sep).rstrip(self.sep).split(self.sep, 1)
            bucket_name = path_list[0]

            if self.bucket:
                bucket = self.bucket
            else:
                try:
                    bucket = self.__conn.get_bucket(bucket_name)
                except S3ResponseError, ins:
                    if ins.code != NO_SUCH_BUCKET:
                        raise
                    if len(path_list) == 1:
                        # COMMENT pathが1階層のみの場合のみ
                        # bucketとして作成する。
                        # 不用意にbucketを
                        # 作成させないようにするため。
                        self.__conn.create_bucket(bucket_name)
                    else:
                        raise

            if len(path_list) == 2:
                key_path = path_list[1]
                key = bucket.new_key(key_path + self.sep)
                key.set_contents_from_string("", reduced_redundancy=self.rrs)

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def get_list(self, bucket=""):
        try:
            if self.bucket:
                return self.bucket.list()
            else:
                return self.__conn.get_bucket(bucket).list()

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def delete_keys(self, bucket=""):
        try:
            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket)

            keys = bucket.list()
            bucket.delete_keys(keys)

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def cancel_halfway_mpupload(self, id="", path="", bucket=""):
        try:
            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket)

            for mp in multipart_upload_lister(bucket):
                if mp.id == id or mp.key_name.encode(self.encoding) == path:
                    mp.cancel_upload()

        except S3ResponseError, ins:
            amazons3_except_handler(ins)

    def _exec_resume(self, mp, file_like, upload_cbs={}):
        _mpupload(mp, file_like, get_part_list(mp), upload_cbs)

    def resume_halfway_mpupload(self, id, file_like, bucket="", upload_cbs={}):
        try:
            if self.bucket:
                bucket = self.bucket
            else:
                bucket = self.__conn.get_bucket(bucket)

            for mp in multipart_upload_lister(bucket):
                if mp.id == id:
                    self._exec_resume(mp, file_like, upload_cbs)

        except S3ResponseError, ins:
            amazons3_except_handler(ins)
